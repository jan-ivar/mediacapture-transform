<pre class='metadata'>
Title: MediaStreamTrack Transform
Shortname: mediacapture-transform
Level: None
Status: UD
Group: webrtc
Repository: jan-ivar/mediacapture-transform
URL: https://jan-ivar.github.io/mediacapture-transform/
Editor: Jan-Ivar Bruaroey, Mozilla https://www.mozilla.org, jib@mozilla.com

Abstract: Proposal <a href="https://github.com/w3c/mediacapture-transform/issues/59">#59</a> by Jan-Ivar Bruaroey with help from Youenn Fablet, written as a future PR off a fork of mediacapture-transform, and thus builds on text from other authors as well. This API defines an API surface for manipulating the bits on {{MediaStreamTrack}}s carrying raw data.
Markup Shorthands: css no, markdown yes
</pre>
<pre class=anchors>
url: https://wicg.github.io/web-codecs/#videoframe; text: VideoFrame; type: interface; spec: WEBCODECS
url: https://wicg.github.io/web-codecs/#videoencoder; text: VideoEncoder; type: interface; spec: WEBCODECS
url: https://wicg.github.io/web-codecs/#audiodata; text: AudioData; type: interface; spec: WEBCODECS
url: https://www.w3.org/TR/mediacapture-streams/#mediastreamtrack; text: MediaStreamTrack; type: interface; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constrainulong; text: ConstrainULong; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constraindouble; text: ConstrainDouble; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-constraindomstring; text: ConstrainDOMString; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-videoresizemodeenum; text: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://w3c.github.io/mediacapture-main/#idl-def-VideoResizeModeEnum.user; text: none; for: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://w3c.github.io/mediacapture-main/#idl-def-VideoResizeModeEnum.right; text: crop-and-scale; for: VideoResizeModeEnum; type: enum; spec: MEDIACAPTURE-STREAMS
url: https://infra.spec.whatwg.org/#queues; text: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#queue-enqueue; text: enqueue; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#queue-dequeue; text: dequeue; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#list-is-empty; text: empty; for: Queue; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#booleans; text: Boolean; type: typedef; spec: INFRA
url: https://www.w3.org/TR/mediacapture-streams/#source-stopped; text: StopSource; type: typedef; spec: MEDIACAPTURE-STREAM
url: https://www.w3.org/TR/mediacapture-streams/#track-ended; text: TrackEnded; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://streams.spec.whatwg.org/#readable-stream-default-controller-close; text: ReadableStreamDefaultControllerClose; type: typedef; spec: STREAMS
url: https://streams.spec.whatwg.org/#readablestream-controller; text: ReadableStreamControllerSlot; type: typedef; spec: STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#ends-nostop; text: EndTrack; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://www.w3.org/TR/mediacapture-streams/#dom-overconstrainederror; text: OverconstrainedError; type: typedef; spec: MEDIACAPTURE-STREAMS
url: https://infra.spec.whatwg.org/#list-empty; text: Empty; for: List; type: typedef; spec: INFRA
url: https://infra.spec.whatwg.org/#list-remove; text: remove; for: List; type: typedef; spec: INFRA
</pre>
<pre class=biblio>
{
  "WEBCODECS": {
     "href":
     "https://wicg.github.io/web-codecs/",
     "title": "WebCodecs"
   },
  "MEDIACAPTURE-SCREEN-SHARE": {
    "href": "https://w3c.github.io/mediacapture-screen-share/",
    "title": "Screen Capture"
  },
  "MEDIACAPTURE-STREAMS": {
    "href": "https://www.w3.org/TR/mediacapture-streams/",
    "title": "Media Capture and Streams"
  },
  "INFRA": {
    "href": "https://https://infra.spec.whatwg.org",
    "title": "Infra"
  },
  "STREAMS": {
    "href": "https://streams.spec.whatwg.org",
    "title": "Streams"
  },
  "WEBAUDIO": {
    "href": "https://www.w3.org/TR/webaudio/",
    "title": "Web Audio API"
  },
  "WEBTRANSPORT": {
    "href": "https://www.w3.org/TR/webtransport/",
    "title": "WebTransport"
  }

}
</pre>
<pre class=link-defaults>
spec:streams; type:interface; text:ReadableStream
spec:streams; type:interface; text:WritableStream
spec:streams; type:dfn; for:/; text:underlying source
spec:streams; type:dfn; for:/; text:underlying sink
</pre>

# Introduction # {#introduction}

The [[WEBRTC-NV-USE-CASES]] document describes several functions that
can only be achieved by access to media (requirements N20-N22),
including, but not limited to:
* Funny Hats
* Machine Learning
* Virtual Reality Gaming

These use cases further require that processing can be done in worker
threads (requirement N23-N24).

This specification gives an interface based on [[WEBCODECS]] and [[STREAMS]] to
provide access to such functionality.

This specification provides access to raw video,
which is the output of a media source such as a camera, screen capture,
or the decoder part of a codec and the input to the
decoder part of a codec. The processed media can be consumed by any destination
that can take a MediaStreamTrack, including HTML &lt;video&gt; and &lt;audio&gt; tags,
RTCPeerConnection, canvas or MediaRecorder.

This specification explicitly aims to support the following use cases:
- *Video processing*: This is the "Funny Hats" use case, where the input is a single video track and the output is a transformed video track.
  - *Multi-source processing*: In this use case, two or more tracks are combined into one. For example, a presentation containing a live weather map and a camera track with the speaker can be combined to produce a weather report application. Audio-visual speech separation, referenced above, is another case of multi-source processing.
  - *Custom video sink*: In this use case, the purpose is not producing a processed {{MediaStreamTrack}}, but to consume the media in a different way. For example, an application could use [[WEBCODECS]] and [[WEBTRANSPORT]] to create an {{RTCPeerConnection}}-like sink, but using different codec configuration and networking protocols.

# Specification # {#specification}

This specification shows the IDL extensions for [[MEDIACAPTURE-STREAMS]].

The API consists of two elements. One is a ReadableStream exposed on a track
that is capable of exposing the unencoded video frames from the track.
The other one is the inverse of that: it provides a track source that takes
video frames as input.

<!-- ## Extension operation ## {#operation} -->

## MediaStreamTrack readable attribute ## {#track-readable}

This specification adds a {{MediaStreamTrack/readable}} {{ReadableStream}}
attribute to {{MediaStreamTrack}}. If the {{MediaStreamTrack}} is a video track,
consumers may read {{VideoFrame}}s from it. The {{MediaStreamTrack}} is the
[=underlying source=] of its {{MediaStreamTrack/readable}} member.

Once the {{ReadableStream}} becomes locked, the {{MediaStreamTrack}} must, in its
role as the [=underlying source=], maintain a circular queue to buffer video frames
coming from the track's source. This buffering allows the {{MediaStreamTrack}}
to temporarily hold frames waiting to be read from its associated {{ReadableStream}}.
The depth of this underlying queue is decided by the UA and can change dynamically.

When a new frame arrives, if the queue is full, the oldest frame will be removed
from the queue, and the new frame will be added to the queue. This means that
for the particular case of a queue with a maximum depth of 1, if there is a
queued frame, it will aways be the most recent one.

The UA is also free to remove any frames from the queue at any time. The UA
may remove frames in order to save resources or to improve performance in
specific situations. In all cases, frames that are not dropped
must be made available to the {{ReadableStream}} in the order in which
they arrive to the {{MediaStreamTrack}}.

The {{MediaStreamTrack/readable}} attribute is only exposed in the
DedicatedWorker context.

### Interface definition ### {#track-interface}

<pre class="idl">
partial interface MediaStreamTrack {
  [Exposed=DedicatedWorker] readonly attribute ReadableStream readable;
};
</pre>

### Internal slots ### {#internal-slots}
<dl>
<dt><dfn for=MediaStreamTrack>`[[queue]]`</dfn></dt>
<dd>A {{Queue|queue}} used to buffer video frames not yet read by the application.</dd>
<dt><dfn for=MediaStreamTrack>`[[numPendingReads]]`</dfn></dt>
<dd>An integer whose value represents the number of read requests issued by the
application that have not yet been handled.
</dd>
<dt><dfn for=MediaStreamTrack>`[[isClosed]]`</dfn></dt>
<dd>An boolean whose value indicates if the {{MediaStreamTrack}} is closed in
its role as [=underlying source=] of its {{readable}} member.
</dd>
</dl>

### Track creation ### {#stream-creation}
At construction of {{MediaStreamTrack}}, run the following steps:
1. Set [=this=].`[[queue]]` to an empty {{Queue}}.
1. Set [=this=].`[[numPendingReads]]` to 0.
1. Set [=this=].`[[isClosed]]` to false.

### Attributes ### {#attributes-track}
<dl>
<dt><dfn for=MediaStreamTrack>readable</dfn></dt>
<dd>Allows reading the frames delivered to the {{MediaStreamTrack}}. This
attribute is created the first time it is invoked according to the following steps:
1. Initialize [=this=].{{MediaStreamTrack/readable}} to be a new {{ReadableStream}}.
2. <a dfn for="ReadableStream">Set up</a> [=this=].{{MediaStreamTrack/readable}} with its [=ReadableStream/set up/pullAlgorithm=] set to [=trackPull=] with [=this=] as parameter, [=ReadableStream/set up/cancelAlgorithm=] set to [=trackCancel=] with [=this=] as parameter, and [=ReadableStream/set up/highWatermark=] set to 0.

The <dfn>trackPull</dfn> algorithm is given a |track| as input. It is defined by the following steps:
1. Increment the value of the |track|.`[[numPendingReads]]` by 1.
2. [=Queue a task=] to run the [=maybeReadFrame=] algorithm with |track| as parameter.
3. Return  [=a promise resolved with=] undefined.

The <dfn>maybeReadFrame</dfn> algorithm is given a |track| as input. It is defined by the following steps:
1. If |track|.`[[queue]]` is {{Queue/empty}}, abort these steps.
2. If |track|.`[[numPendingReads]]` equals zero, abort these steps.
3. {{Queue/dequeue}} a frame from |track|.`[[queue]]` and [=ReadableStream/Enqueue=] it in |track|.{{MediaStreamTrack/readable}}.
4. Decrement |track|.`[[numPendingReads]]` by 1.
5. Go to step 1.

The <dfn>trackCancel</dfn> algorithm is given a |track| as input.
It is defined by running the following steps:
1. Run the [=trackClose=] algorithm with |track| as parameter.
3. Return  [=a promise resolved with=] undefined.

The <dfn>trackClose</dfn> algorithm is given a |track| as input.
It is defined by running the following steps:
1. If |track|.`[[isClosed]]` is true, abort these steps.
3. {{ReadableStreamDefaultControllerClose|Close}} |track|.{{MediaStreamTrack/readable}}.{{ReadableStreamControllerSlot|[[controller]]}}.
4. {{List/Empty}} |track|.`[[queue]]`.
5. Set |track|.`[[isClosed]]` to true.

</dd>
</dl>

### Handling interaction with the track ### {#track-interaction}
When a {{MediaStreamTrack}} |track| receives a video frame, the UA MUST execute
the [=handleNewFrame=] algorithm with |track| as parameter.

The <dfn>handleNewFrame</dfn> algorithm is given a |track| as input.
It is defined by running the following steps:
1. If |track|.`[[queue]]` has is full, as determined b the UA, {{Queue/dequeue}}
   an item from |track|.`[[queue]]`.
2. {{Queue/enqueue}} the new frame in |track|.`[[queue]]`.
3. [=Queue a task=] to run the [=maybeReadFrame=] algorithm with |track| as parameter.

At any time, the UA MAY {{List/remove}} any frame from |track|.`[[queue]]`.
The UA may decide to remove frames from |track|.`[[queue]]`, for example,
to prevent resource exhaustion or to improve performance in certain situations.
</dd>

<p class="note">
The application may detect that frames have been dropped by noticing that there
is a gap in the timestamps of the frames.
</p>
</dl>

When a {{MediaStreamTrack}} |track| {{TrackEnded|ends}}, the [=trackClose=]
algorithm must be executed with |track| as parameter.

## VideoTrackSource ## {#video-track-source}
A {{VideoTrackSource}} allows the creation of a video source for a
{{MediaStreamTrack}} in the
<a href="https://www.w3.org/TR/mediacapture-streams/#the-model-sources-sinks-constraints-and-settings">
MediaStream model</a>. It has a two readonly attributes: a
{{VideoTrackSource/writable}} {{WritableStream}} and a {{VideoTrackSource/track}}
{{MediaStreamTrack}}.

The {{VideoTrackSource}} is the [=underlying sink=] of its
{{VideoTrackSource/writable}} attribute. The {{VideoTrackSource/track}} attribute
is the output. Further tracks connected to the same {{VideoTrackSource}} can be
created using the {{MediaStreamTrack/clone}} method on the
{{VideoTrackSource/track}} attribute.

The {{WritableStream}} accepts {{VideoFrame}} objects.
When a {{VideoFrame}} is written to {{VideoTrackSource/writable}},
the frame's `close()` method is automatically invoked, so that its internal
resources are no longer accessible from JavaScript.

A {{VideoTrackSource}} object only exists in the DedicatedWorker context.
However, the {{MediaStreamtrack}}(s) it sources may be transferred to other
contexts.

### Interface definition ### {#video-source-interface}
<pre class="idl">
[Exposed=DedicatedWorker]
interface VideoTrackSource {
  constructor();
  readonly attribute WritableStream writable;
  attribute boolean muted;
  readonly attribute MediaStreamTrack track;
}
</pre>

### Internal slots ### {#internal-slots}
<dl>
<dt><dfn for=VideoTrackSource>`[[track]]`</dfn></dt>
<dd>The {{MediaStreamTrack}} output of this source</dd>
<dt><dfn for=VideoTrackSource>`[[isMuted]]`</dfn></dt>
<dd>A boolean whose value indicates whether this source and all the
{{MediaStreamTrack}}s it sources, are currently {{MediaStreamTrack/muted}} or not.
</dd>
</dl>

### Constructor ### {#video-source-constructor}
<dfn constructor for=VideoTrackSource title="VideoTrackSource(init)">
  VideoTrackSource()
</dfn>
1. Let |source| be a new {{VideoTrackSource}} object.
1. Let |track| be a new {{MediaStreamTrack}} object, whose
   {{MediaStreamTrack/kind}} is "video", and whose {{MediaStreamTrack/id}} is a
   new unique id generated by the user agent.
1. Initialize |source|.{{VideoTrackSource/track}} to |track|.
4. Return |source|.

### Attributes ### {#video-source-attributes}
<dl>
<dt><dfn attribute for=VideoTrackSource>writable</dfn></dt>
<dd>Allows writing video frames to the {{VideoTrackSource}}. When this attribute
is accessed for the first time, it MUST be initialized with the following steps:
1. Initialize [=this=].{{VideoTrackSource/writable}} to be a new {{WritableStream}}.
2. <a dfn for="WritableStream">Set up</a> [=this=].{{VideoTrackSource/writable}}, with its [=WritableStream/set up/writeAlgorithm=] set to [=writeFrame=] with |this| as parameter, with [=WritableStream/set up/closeAlgorithm=] set to [=closeWritable=] with |this| as parameter and [=WritableStream/set up/abortAlgorithm=] set to [=closeWritable=] with |this| as parameter.

The <dfn>writeFrame</dfn> algorithm is given a |source| and a |frame| as input. It is defined by running the following steps:
1. If |frame| is not a {{VideoFrame}} object, return [=a promise rejected with=]
   a {{TypeError}}.
1. If |source|.`[[isMuted]]` is false, send the media data backing |frame| to
   all live tracks sourced from |source|.
1. Invoke the `close` method of |frame|.
1. Return [=a promise resolved with=] undefined.

<p class="note">
When the media data is sent to a track, the UA may apply processing
(e.g., cropping and downscaling) to ensure that the media data sent
to the track satisfies the track's constraints. Each track may receive a
different version of the media data depending on its constraints.
</p>

The <dfn>closeWritable</dfn> algorithm is given a |source| as input.
It is defined by running the following steps.
1. For each track `t` sourced from |source|, {{EndTrack|end}} `t`.
2. Return [=a promise resolved with=] undefined.

</dd>
<dt><dfn attribute for=VideoTrackSource>muted</dfn></dt>
<dd>Mutes the {{VideoTrackSource}}. The getter steps are to return
[=this=].`[[isMuted]]`. The setter steps, given a value |newValue|, are as follows:
1. If |newValue| is equal to [=this=].`[[isMuted]]`, abort these steps.
1. Set [=this=].`[[isMuted]]` to |newValue|.
1. Unless one has been queued already this run of the event loop,
   [=queue a task=] to run the following steps:
    1. Let |settledValue| be [=this=].`[[isMuted]]`.
    1. For each live track sourced by [=this=], [=queue a task=] to
       [=set a track's muted state=] to |settledValue|.
</dd>
<dt><dfn attribute for=VideoTrackSource>track</dfn></dt>
<dd>The {{MediaStreamTrack}} output. The getter steps are to return
[=this=].`[[track]]`.
</dd>
</dl>

### Specialization of MediaStreamTrack behavior ### {#video-source-as-track}
A {{VideoTrackSource}} acts as the source for one or more {{MediaStreamTrack}}s.
This section adds clarifications on how a {{MediaStreamTrack}} sourced from a
{{VideoTrackSource}} behaves.

#### stop #### {#video-source-stop}
The {{MediaStreamTrack/stop}} method stops the track. When the last track
sourced from a {{VideoTrackSource}} ends, that {{VideoTrackSource}}'s
{{VideoTrackSource/writable}} is [=WritableStream/closing|closed=].

#### Constrainable properties #### {#video-source-constrainable-properties}

The following constrainable properties are defined for {{MediaStreamTrack}}s
sourced from a {{VideoTrackSource}}:
<table>
  <thead>
    <tr>
      <th>
        Property Name
      </th>
      <th>
        Values
      </th>
      <th>
        Notes
      </th>
    </tr>
  </thead>
  <tbody>
    <tr id="def-constraint-width">
      <td data-tests="">
        <dfn>width</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the width, in pixels, of the latest
        frame received by the track.
        As a capability, `max` MUST reflect the
        largest width a {{VideoFrame}} may have, and `min`
        MUST reflect the smallest width a {{VideoFrame}} may have.
      </td>
    </tr>
    <tr id="def-constraint-height">
      <td data-tests="">
        <dfn>height</dfn>
      </td>
      <td>
        {{ConstrainULong}}
      </td>
      <td>
        As a setting, this is the height, in pixels, of the latest
        frame received by the track.
        As a capability, `max` MUST reflect the largest height
        a {{VideoFrame}} may have, and `min` MUST reflect
        the smallest height a {{VideoFrame}} may have.
      </td>
    </tr>
    <tr id="def-constraint-frameRate">
      <td data-tests="">
        <dfn>frameRate</dfn>
      </td>
      <td>
        {{ConstrainDouble}}
      </td>
      <td>
        As a setting, this is an estimate of the frame rate based on frames
        recently received by the track.
        As a capability `min` MUST be zero and
        `max` MUST be the maximum frame rate supported by the system.
      </td>
    </tr>
    <tr id="def-constraint-aspect">
      <td data-tests="">
        <dfn>aspectRatio</dfn>
      </td>
      <td>
        {{ConstrainDouble}}
      </td>
      <td>
        As a setting, this is the aspect ratio of the latest frame
        delivered by the track;
        this is the width in pixels divided by height in pixels as a
        double rounded to the tenth decimal place. As a capability,
        `min` MUST be the
        smallest aspect ratio supported by a {{VideoFrame}}, and `max` MUST be
        the largest aspect ratio supported by a {{VideoFrame}}.
      </td>
    </tr>
    <tr id="def-constraint-resizeMode">
      <td data-tests="">
        <dfn>resizeMode</dfn>
      </td>
      <td>
        {{ConstrainDOMString}}
      </td>
      <td>
        As a setting, this string should be one of the members of
        {{VideoResizeModeEnum}}. The value "{{VideoResizeModeEnum/none}}"
        means that the frames output by the MediaStreamTrack are unmodified
        versions of the frames written to the
        {{VideoTrackSource/writable}} backing
        the track, regardless of any constraints.
        The value "{{VideoResizeModeEnum/crop-and-scale}}" means
        that the frames output by the MediaStreamTrack may be cropped and/or
        downscaled versions
        of the source frames, based on the values of the width, height and
        aspectRatio constraints of the track.
        As a capability, the values "{{VideoResizeModeEnum/none}}" and
        "{{VideoResizeModeEnum/crop-and-scale}}" both MUST be present.
      </td>
    </tr>
  </tbody>
</table>

The {{MediaStreamTrack/applyConstraints}} method applied to a video {{MediaStreamTrack}}
sourced from a {{VideoTrackSource}} supports the properties defined above.
It can be used, for example, to resize frames or adjust the frame rate of the track.
Note that these constraints have no effect on the {{VideoFrame}} objects
written to the {{VideoTrackSource/writable}} of a {{VideoTrackSource}},
just on the output of the track on which the constraints have been applied.
Note also that, since a {{VideoTrackSource}} can in principle produce
media data with any setting for the supported constrainable properties,
an {{MediaStreamTrack/applyConstraints}} call on a track
backed by a {{VideoTrackSource}} will generally not fail with
{{OverconstrainedError}} unless the given constraints
are outside the system-supported range, as reported by
{{MediaStreamTrack/getCapabilities}}.

#### Events and attributes #### {#video-source-events-attributes}
Events and attributes work the same as for any {{MediaStreamTrack}}.
It is relevant to note that if the {{VideoTrackSource/writable}}
stream of a {{VideoTrackSource}} is closed, all the live
tracks connected to it are ended and the `ended` event is fired on them.

# Examples # {#examples}
## Video Processing
Consider a face recognition function `detectFace(videoFrame)` that returns a face position
(in some format), and a manipulation function `blurBackground(videoFrame, facePosition)` that
returns a new VideoFrame similar to the given `videoFrame`, but with the
non-face parts blurred. The example also shows the video before and after
effects on video elements.

<pre class="example" highlight="js">
// main.js

const stream = await navigator.mediaDevices.getUserMedia({video:true});
const videoBefore = document.getElementById('video-before');
const videoAfter = document.getElementById('video-after');
videoBefore.srcObject = stream.clone();

const [track] = stream.getVideoTracks();
const worker = new Worker('worker.js');
worker.postMessage({track}, [track]);

const {data} = await new Promise(r => worker.onmessage);
videoAfter.srcObject = new MediaStream([data.track]);

// worker.js

self.onmessage = async ({data: {track}}) => {
  const source = new VideoTrackSource();
  parent.postMessage({track: source.track}, [source.track]);

  const transformer = new TransformStream({
    async transform(frame, controller) {
      const facePosition = await detectFace(frame);
      const newFrame = blurBackground(frame, facePosition);
      frame.close();
      controller.enqueue(newFrame);
    }
  });
  await track.readable.pipeThrough(transformer).pipeTo(source.writable);
};
</pre>

## Multi-consumer post-processing with constraints
A common use case is to remove the background from live camera video fed into a
video conference, with a live self-view showing the result. It's desirable for
the self-view to be smooth even if the frame rate used for actual sending may
dip lower due to bandwidth constraints. This can be solved using clone() and
applyConstraints(), without having to process twice.

<pre class="example" highlight="js">
// main.js

const stream = await navigator.mediaDevices.getUserMedia({video:true});
const [track] = stream.getVideoTracks();
const worker = new Worker('worker.js');
worker.postMessage({track}, [track]);

const {data} = await new Promise(r => worker.onmessage);
const selfView = document.getElementById('video-self');
selfView.srcObject = new MediaStream([data.track.clone()]); // 60 fps

await data.track.applyConstraints({width: 320, height: 200, frameRate: 30});
const pc = new RTCPeerConnection(config);
pc.addTrack(data.track); // 30 fps

// worker.js

self.onmessage = async ({data: {track}}) => {
  const source = new VideoTrackSource();
  parent.postMessage({track: source.track}, [source.track]);

  const transformer = new TransformStream({transform: myRemoveBackgroundFromVideo});
  await track.readable.pipeThrough(transformer).pipeTo(source.writable);
};
</pre>

## Multi-consumer post-processing with constraints in a worker
Being able to show a higher frame-rate self-view is also relevant when sending
video frames over WebTransport. The same technique above may be used here,
except clone() and applyConstraints() happen in the worker.

<pre class="example" highlight="js">
// main.js

const stream = await navigator.mediaDevices.getUserMedia({video:true});
const [track] = stream.getVideoTracks();
const worker = new Worker('worker.js');
worker.postMessage({track}, [track]);

const {data} = await new Promise(r => worker.onmessage);
const selfView = document.getElementById('video-self');
selfView.srcObject = new MediaStream([data.track]); // 60 fps

// worker.js

self.onmessage = async ({data: {track}}) => {
  const source = new VideoTrackSource();
  const sendTrack = source.track.clone();
  parent.postMessage({track: source.track}, [source.track]);

  await sendTrack.applyConstraints({width: 320, height: 200, frameRate: 30});

  const wt = new WebTransport("https://webtransport.org:8080/up");

  const transformer = new TransformStream({transform: myRemoveBackgroundFromVideo});
  await Promise.all([
    track.readable.pipeThrough(transformer).pipeTo(source.writable),
    sendTrack.readable.pipeTo(wt.createUnidirectionalStream()) // 30 fps
  ]);
};
</pre>
<div class="note">
<p>The above example avoids using the `tee()` function to serve multiple
consumers, due to its issues with real-time streams.</p>
<p>For brevity, the example also over-simplifies sending video frames over
WebTransport (incurring head-of-line blocking).</p>
</div>

# Implementation advice # {#implementation-advice}

This section is informative.

## Use with multiple consumers

There are use cases where the programmer may desire that a single stream of frames
is consumed by multiple consumers.

Examples include the case where the result of a background blurring function should
be both displayed in a self-view and encoded using a {{VideoEncoder}}.

For cases where both consumers are consuming unprocessed frames, and synchronization
is not desired, instantianting multiple {{MediaStreamTrack}} clones is a robust solution.

For cases where both consumers intend to convert the result of a processing step into a
{{MediaStreamTrack}}
using a {{VideoTrackSource}}, for example when feeding a processed stream
to both a &lt;video&gt& tag and an {{RTCPeerConnection}}, attaching the resulting {{MediaStreamTrack}}
to multiple sinks may be the most appropriate mechanism.

For cases where the downstream processing takes frames, not streams, the frames can
be cloned as needed and sent off to the downstream processing; "clone" is a cheap operation.

When the stream is the output of some processing, and both branches need a Stream object
to do further processing, one needs a function that produces two streams from one stream.

However, the standard tee() operation is problematic
in this context:

*   It defeats the backpressure mechanism that guards against excessive queueing
*   It creates multiple links to the same buffers, meaning that the question of which
    consumer gets to destroy() the buffer is a difficult one to address

Therefore, the use of tee() with Streams containing media should only be done when
fully understanding the implications. Instead, custom elements for splitting streams
more appropriate to the use case should be used.

*   If both branches require the ability to dispose of the frames, clone() the frame
    and enqueue distinct copies in both queues. This corresponds to the function
    ReadableStreamTee(stream, cloneForBranch2=true). Then choose one of the
    alternatives below.

*   If one branch requires all frames, and the other branch tolerates dropped frames,
    enqueue buffers in the all-frames-required stream and use the backpressure signal
    from that stream to stop reading from the source. If backpressure signal from the
    other stream indicates room, enqueue the same frame in that queue too.

*   If neither stream tolerates dropped frames, use the combined backpressure signal
    to stop reading from the source. In this case, frames will be processed in
    lockstep if the buffer sizes are both 1.

*   If it is OK for the incoming stream to be stalled only when the underlying
    buffer pool allocated to the process is exhausted, standard tee() may be used.

# Security and Privacy considerations # {#security-considerations}

This API exposes data access to and from {{MediaStreamTrack}}.

The security and privacy of {{VideoTrackSource}} relies on the same-origin
policy. That is, the data {{VideoTrackSource}} can make available in the form of
a {{MediaStreamTrack}} is visible to the document before a {{VideoFrame}} can be
constructed and written to the {{VideoTrackSource}}. Any attempt to create
{{VideoFrame}} objects using cross-origin data will fail. Therefore,
{{VideoTrackSource}} does not introduce any new fingerprinting surface.

The {{MediaStreamTrack}}'s {{MediaStreamTrack/readable}} attribute introduced by
this API exposes the same data that is exposed by other {{MediaStreamTrack}}
sinks such as {{RTCPeerConnection}} and media elements. The security and privacy
of {{MediaStreamTrack}}'s {{MediaStreamTrack/readable}} relies on the security
and privacy of {{MediaStreamTrack}}. For example, camera, microphone and
screen-capture tracks rely on explicit use authorization via permission dialogs
(see [[MEDIACAPTURE-STREAMS]] and [[MEDIACAPTURE-SCREEN-SHARE]]), while element
capture and {{VideoTrackSource}} rely on the same-origin policy.
A potential issue with {{MediaStreamTrack}}'s {{MediaStreamTrack/readable}} is
resource exhaustion.
For example, a site might hold on to too many open {{VideoFrame}} objects
and deplete a system-wide pool of GPU-memory-backed frames. UAs can
mitigate this risk by limiting the number of pool-backed frames a site can
hold. This can be achieved by reducing the maximum number of buffered frames
and by refusing to deliver more frames to the {{MediaStreamTrack/readable}}
once the budget limit is reached. Accidental exhaustion is also mitigated by
automatic closing of {{VideoFrame}} objects once they
are written to a {{VideoTrackSource}}.
